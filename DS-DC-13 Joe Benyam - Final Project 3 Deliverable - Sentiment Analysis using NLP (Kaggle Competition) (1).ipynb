{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Benyam\n",
    "DS/DS/13 Final Project 3 Deliverable\n",
    "\n",
    "  ### Subject: Sentiment Analysis of ISIS Social Network Strategy\n",
    "  (https://www.kaggle.com/kzaman/how-isis-uses-twitter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem: \n",
    "Terrorist organizations continue to use social media as a prominent tool for recruitment. ISIS Fanboys use common social media platforms like Twitter to carry out radicalization campaigns in hopes of inspiring terror. However, counter-terrorism analysts can apply emerging Natural Language Processing (NLP) Teqniques to gain deeper insights from ISIS Fanboy-generated social media intelligence.\n",
    "\n",
    "#### Research Question: \n",
    "Using Natural Language Processing, statistics, or machine learning methods to extract, identify, or otherwise characterize the sentiment content of the following prominent islamic clergy:\n",
    "\n",
    "###### Like the Most (Positive Sentiment):\n",
    "     \"Anwar Awlaki\", \n",
    "     \"Ahmad Jibril\", \n",
    "     \"Ibn Taymiyyah\", \n",
    "     \"Abdul Wahhab\"\n",
    " \n",
    "###### Hate the Most (Negative Sentiment):\n",
    "     \"Hamza Yusuf\", \n",
    "     \"Suhaib Webb\", \n",
    "     \"Yaser Qadhi\", \n",
    "     \"Nouman Ali Khan\", \n",
    "     \"Yaqoubi\".\n",
    "\n",
    "#### Hypothesis: \n",
    "I plan to determine which clergy do pro-ISIS fanboys like the most and which ones do they hate the most. Using Natural Language Processing methods to extract, identify, or otherwise characterize the sentiment of the named islamic religious leaders. I will provide brief descriptions of each Islamic clergy for context in understanding the sentiments. This domaine knowledge is critical for effective implementation of sentiment analysis, particularly to bettercomprehend the context.\n",
    "\n",
    "#### Research Data: \n",
    "The dataset consists of 17,000 tweets from 100+ pro-ISIS sympathizers from all over the world since November 2016 Paris Attacks. The dataset is structured and includes.the following labels: name, username, description, location, Number of followers at the time the tweet was downloaded, Number of statuses by the user when the tweet was downloaded, date and timestamp of the tweet, the tweet itself. However, we can conduct out sentiment analysis with an aggreagation of the the tweets.\n",
    "\n",
    "#### Tools and Keywords: \n",
    "spaCy, Gensim, word2vec, Natural Language Processing, Unstructured Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preliminary Steps and Twitter Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import DS natural language processing tools - Spacy and Gensim (complete)\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the twitter data (complete)\n",
    "df = pd.read_csv(\"/Users/josephbenyam/Downloads/tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>followers</th>\n",
       "      <th>numberstatuses</th>\n",
       "      <th>time</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:07</td>\n",
       "      <td>ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:27</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:29</td>\n",
       "      <td>ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:37</td>\n",
       "      <td>ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:45</td>\n",
       "      <td>ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 21:51</td>\n",
       "      <td>THE SECOND CLIP IN A DA'WAH SERIES BY A SOLDIE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 22:04</td>\n",
       "      <td>ENGLISH TRANSCRIPT : OH MURABIT! : http://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 22:06</td>\n",
       "      <td>ENGLISH TRANSLATION: 'A COLLECTION OF THE WORD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/6/2015 22:17</td>\n",
       "      <td>Aslm Please share our new account after the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GunsandCoffee</td>\n",
       "      <td>GunsandCoffee70</td>\n",
       "      <td>ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640</td>\n",
       "      <td>49</td>\n",
       "      <td>1/10/2015 0:05</td>\n",
       "      <td>ENGLISH TRANSLATION: AQAP STATEMENT REGARDING ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name         username  \\\n",
       "0  GunsandCoffee  GunsandCoffee70   \n",
       "1  GunsandCoffee  GunsandCoffee70   \n",
       "2  GunsandCoffee  GunsandCoffee70   \n",
       "3  GunsandCoffee  GunsandCoffee70   \n",
       "4  GunsandCoffee  GunsandCoffee70   \n",
       "5  GunsandCoffee  GunsandCoffee70   \n",
       "6  GunsandCoffee  GunsandCoffee70   \n",
       "7  GunsandCoffee  GunsandCoffee70   \n",
       "8  GunsandCoffee  GunsandCoffee70   \n",
       "9  GunsandCoffee  GunsandCoffee70   \n",
       "\n",
       "                                    description location  followers  \\\n",
       "0  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "1  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "2  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "3  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "4  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "5  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "6  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "7  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "8  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "9  ENGLISH TRANSLATIONS: http://t.co/QLdJ0ftews      NaN        640   \n",
       "\n",
       "   numberstatuses            time  \\\n",
       "0              49  1/6/2015 21:07   \n",
       "1              49  1/6/2015 21:27   \n",
       "2              49  1/6/2015 21:29   \n",
       "3              49  1/6/2015 21:37   \n",
       "4              49  1/6/2015 21:45   \n",
       "5              49  1/6/2015 21:51   \n",
       "6              49  1/6/2015 22:04   \n",
       "7              49  1/6/2015 22:06   \n",
       "8              49  1/6/2015 22:17   \n",
       "9              49  1/10/2015 0:05   \n",
       "\n",
       "                                              tweets  \n",
       "0  ENGLISH TRANSLATION: 'A MESSAGE TO THE TRUTHFU...  \n",
       "1  ENGLISH TRANSLATION: SHEIKH FATIH AL JAWLANI '...  \n",
       "2  ENGLISH TRANSLATION: FIRST AUDIO MEETING WITH ...  \n",
       "3  ENGLISH TRANSLATION: SHEIKH NASIR AL WUHAYSHI ...  \n",
       "4  ENGLISH TRANSLATION: AQAP: 'RESPONSE TO SHEIKH...  \n",
       "5  THE SECOND CLIP IN A DA'WAH SERIES BY A SOLDIE...  \n",
       "6  ENGLISH TRANSCRIPT : OH MURABIT! : http://t.co...  \n",
       "7  ENGLISH TRANSLATION: 'A COLLECTION OF THE WORD...  \n",
       "8  Aslm Please share our new account after the pr...  \n",
       "9  ENGLISH TRANSLATION: AQAP STATEMENT REGARDING ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 66-69: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8179a99731e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m             \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62658)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-8179a99731e3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 66-69: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "df2['tweets'] = df['tweets'].apply(lambda x: str(x).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis and Target Descriptions (NLP Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Abstract:\n",
    "Using Natural Language Processing (NLP) teqniques, identify and print tweets containing named muslim clergy. Provide brief description of the muslim clergy for to help the reader better evaluate the sentiments of the ISIS fanboys. Order by examples of clergy they like the most and examples of clergy they hate the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-83ac7923d726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparsed_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tweets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparsed_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_toolkit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, tag, parse, entity)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'An'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__ (spacy/tokenizer.cpp:4835)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mcache_hit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_hit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_ws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize (spacy/tokenizer.cpp:5409)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0morig_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_affixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens (spacy/tokenizer.cpp:6444)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0minfix_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                         \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minfix_start\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                         \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0minfix_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfix_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minfix_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.push_back (spacy/tokens/doc.cpp:8356)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parse each tweet (complete)\n",
    "parsed_tweets = []\n",
    "for tweets in df[\"tweets\"]:\n",
    "    parsed_tweets.append(nlp_toolkit(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a  function that can take a sentence prased by spacy and identify if it mentions a named-Sheik.\n",
    "def sheiks_of_interest(parsed, person):\n",
    "    for entity in parsed.ents:\n",
    "        if entity.text == person and entity.label_=='PERSON':\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheik's ISIS Fanboys Like the Most (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ahmad Jibril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "Contemporary Salafist extremist born on 1971 in Dearborn, Michigan. Active recruiter and propogandist. Strong reverent of Salafist Ideologies. Ahman Jibril reportedly advocates for Westerners to serve as foriegn fighters in the war in Syria. He directly praises Syrian rebeles, comparing them to early muslim fighters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets \n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Ahmad Jibril'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Finding: \n",
    "One instance of Shaykh Ahmad Jibril referenced in the dataset. ISIS Fanboy conveys solidarity with neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Anwar Alawki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets:\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Anwar Alawki'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Finding\n",
    "The absence of any return on the above function call suggests that Abdul Wahhab is not a named within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ibn Taymiyya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brief Description:\n",
    "Notable 14th century islamic religious scholar. Espoused the famous Faqi form of Islamic jurisprudence. Believed that Islamic form of government can vary from time to time and place to place depending on custom and circumstances, as long as Shariah is safeguarded.\n",
    "His main issues:\n",
    "\n",
    "Shifted the focus of jihad from personal and social to military (politization of jihad) *this was not a theme of reform Necessity of maintaining the flexibility of Islamic law through ijtihad  *a theme of reformà to make sure that its government is still Islamic.\n",
    "\n",
    "Rejecting the ethos of fatalism: The opposite of ijtihad is following precedent.  The idea that when you are living in a society that says that everything is fine and you should just accept it because it is God’s will traces back to the time when.\n",
    " \n",
    "His main doctrine was, in Ḥanbalī fashion, based on the supremacy of Qurʿān and sunnah (received custom) and the salaf (early Muslims)as ultimate authorities.  He thought that the form of government can vary but legal authority does not only remain distinct from executive administration but is also of primary importance: an Islamic legal system must be maintained to give the government legitimacy.  \n",
    "\n",
    "Believed that many of the problems of the Muslim world in his time resulted from leaders’ efforts to keep that world politically unified.  Ideally, the community was united, but in reality, it was divided into regional units.  \n",
    "\n",
    "He was distrustful of non-Mislims but insisted on religious freedom.  Rejection of fatalism and rejection of intercession of the saints.  Argued that ijtihad must remain active and was a religious duty, lest Islamic law become irrelevant.  He stressed the difference between Shariah and fiqh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @IWantDollars: Ibn Taymiyya condemned Hulago Khan in his face. But Celebrity Mufti Ismail Menk presents gifts to occupiers instead: http\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test Function Call and Print Tweets: \n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Ibn Taymiyya'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expirement Findings:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Muhammed Abdul Wahhab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description: \n",
    "Founder of the fundamentalist sect of Islamic tradition that believed in reverting to the values of the original Islamic community. He tought a traditional and literal application of the Quran, which focused on mirroring early Isalmic practices, depite reforms in traditions, morals, and technology. Wahhabis strictly prohibit innovation and intepretation of messages within the Quran.\n",
    "\n",
    "He also believed in Tafkir, which is the practice of labeling someone an unbeliever, which ironically was prohibited in the fist Islamic state. He sanction the destruction of shrines and temples and violently opressed/sometimes massacred Sufis and Shia muslims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Abdul Wahhab'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Finding: \n",
    "The absence of any return on the above function call suggests that Abdul Wahhab is not a named within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheik's ISIS Fanboys Hate the Most (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hamza Yusuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets: \n",
    "## Again, the absence of any return below indicates that Muhammed Abdul Wahhab is\n",
    "## not a named entity within the dataset.\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Hamza Yusuf'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Finding: \n",
    "The absence of any return on the above function call suggests that Hamza Yusuf is not a named Persons entity within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Suhaib Webb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @IWantDollars: Suhaib Webb prays for dead Kuffar but is silent on the 100+ students killed in Mosul University. No Walaa wal Baraa: http\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test Function Call and Print Tweets\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Suhaib Webb'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Findings: \n",
    "The Islamic State on Tuesday claimed responsibility for bombings that killed at least 34 people at the Brussels airport and a subway station. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Yaser Qadhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets:\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Yaser Qadhi'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Findings: \n",
    "The absence of any return above indicates that Yaser Qadhi is not a named entity within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Nouman Ali Khan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets: \n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Nouman Ali Khan'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Finding: \n",
    "The absence of any return on the above function call suggests that Yaqoubi is not a named persons entity within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Yaqoubi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Description:\n",
    "*Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test Function Call and Print Tweets\n",
    "for i, parsed_tweet in enumerate(parsed_tweets):\n",
    "    if sheiks_of_interest(parsed_tweet, 'Yaqoubi'):\n",
    "        print parsed_tweet\n",
    "        if i>1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Finding: \n",
    "The absence of any return on the above function call suggests that Yaqoubi is not a named Persons entity within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
